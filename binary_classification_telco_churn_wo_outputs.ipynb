{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM:\n",
    "Use the example data set to answer following questions:\n",
    "1. Which customers are most likely to churn?\n",
    "2. What can we do to reduce churn?\n",
    "3. Present your answers in a format you think is most appropriate for your stakeholders to get\n",
    "your message across.\n",
    "4. You can use whatever software and method you find appropriate to do your analysis, and\n",
    "submit your code, notebook, markdown, github repo, or any format you used for your\n",
    "analysis.\n",
    "5. Any other findings in addition to above question 1 & 2 you want to share from your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "\n",
    "The purpose of this notebook is to demonstrate how we can use the data provided by the company/client to come up with actionable insights and recommendations to address their problem of customer retention. \n",
    "\n",
    "There could be multiple ways to solve this problem. What is proposed below is just one of the ways. Note - the emphasis is not on machine learning algorithms or techniques but a framework to solve the data probem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "width = 3\n",
    "height = 3\n",
    "plt.figure(figsize=(width, height))\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from featexp import get_univariate_plots\n",
    "from featexp import get_trend_stats\n",
    "\n",
    "from feature_selector import FeatureSelector\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, f1_score, classification_report, precision_score, recall_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "random.seed(3)\n",
    "\n",
    "from pdpbox import info_plots, get_dataset\n",
    "\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import shap\n",
    "shap.initjs() \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import iml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "    \n",
    "Bringing together and assembling the data that you see in the spreadsheet could be a massive exercise by itself. This will likely involve pulling data from different data sources (databases, tables, flat files etc.), performing data quality\n",
    "checks and aggregations, if required, before consolidating it to a single dataset. Further, the analyst will have to engage with stakeholders and the business to define the target variable (i.e. the outcome of interest) based on the business problem. In this case, the target variable 'Churn' has already been defined for us, which means company losing customers or customers stopping to pay for the services. \n",
    "\n",
    "Customer can stop doing business with a company and switch to a different one due to multiple reasons such as price, customer service, product features etc. This is called voluntary churn. Customers could also churn for other reasons such as credit card expiry, falling behind payments etc. and this is called involunatry churn. As you can see, these are 2 different problems altogether. Therefore, it is important to understand the business problem clearly and define the target variable accordingly.\n",
    "\n",
    "For this problem, let's make the assumption that customer is churning because of the shortcomings on the company's part - i.e. voluntary churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv file\n",
    "file_name = 'C:/Users/Lenovo/Desktop/churn.csv' # Provide the path of the csv file here based on where you save it\n",
    "df = pd.read_csv(file_name,na_values=' ') # Attribute 'totalcharges' has blank values and hence converted as string while importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of imported dataframe\n",
    "df.shape\n",
    "print (\"In the imported dataframe, the number of observations is {rows} and the number of variables is {cols}\".format(rows= df.shape[0],cols= df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all columns names to lower-case for consistency and standardization\n",
    "df.columns = map(str.lower, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek at the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "'customerid' is the identifier variable\n",
    "\n",
    "'churn' is the target variable\n",
    "\n",
    "There is a combination of categorical and continuous variables in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nulls and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'total charges' seems to have very few missing values. Let's check this later. Format of all the variables look alright and as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "    \n",
    "If the data types don't look okay, they need to be converted from chararcter to numeric or vice versa. For eg. postcode or zipcode has to be converted to character if it is read as numeric while/after importing into a dataframe and then treated or transformed based on how you want to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "df['customerid'].nunique()\n",
    "print (\"The number of duplicates in the dataframe is {num_dupes}\".format(num_dupes = len(df['customerid']) - df['customerid'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target variable to binary\n",
    "df.loc[df.churn=='Yes','churn'] = 1\n",
    "df.loc[df.churn=='No','churn'] = 0\n",
    "df['churn'] = df['churn'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "target_dist = pd.value_counts(df['churn'],normalize=True)\n",
    "print (\"The churn rate in the population is {:.2f}%\".format(target_dist.iloc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "\n",
    "Not sure what the churn benchmarks are in the telecom industry but ~27% is high. The company seems to be facing serious challenges with customer retention as around 3 out of 10 customers are leaving!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing value distribution\n",
    "print (\"Missing value distribution in dataframe\")\n",
    "df.isna().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "\n",
    "'totalcharges' column has 11 missing values (~ 0.15%). We will choose a suitable imputation strategy, if required, post EDA\n",
    "\n",
    "Note - Unlike this dataset, which is clean, real life datasets are dirty and a lot of data pre-processing will likely be required to bring ito into a shape that is suitable to build a model on. This data cleansing process generally involves ~70-80% of time and effort in a data science engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of different datatypes\n",
    "df.drop(['customerid','churn'],axis=1).dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of features in the dataframe are categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of unique values for each of the variables\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "Let's convert 'seniorcitizen', which is currently binary, into categorical for EDA.\n",
    "\n",
    "This dataset doesn't have categorical variables with high cardinality (i.e. too many categories). Look up on this topic on the web to familiarize yourself on ways to treat data with high cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency distribution of seniorcitizen variable\n",
    "df['seniorcitizen'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert seniorcitizen to categorical variable for purposes of EDA\n",
    "df.loc[df.seniorcitizen== 1,'seniorcitizen'] = 'Yes'\n",
    "df.loc[df.seniorcitizen== 0,'seniorcitizen'] = 'No'\n",
    "df['seniorcitizen'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign identifier and target variables\n",
    "id_col = ['customerid']\n",
    "target_col = ['churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analaysis (EDA) - categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plots for all categorical variables\n",
    "# %matplotlib inline\n",
    "# cat_cols = [x for x in df.select_dtypes(include=['object', 'category']).columns if x not in id_col if x not in target_col]\n",
    "# for col in cat_cols:\n",
    "#     sns.catplot(x=col, kind=\"count\", data=df)\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.title(\"\\n\\n\\n\\n Count plot for {}\".format(col))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plots for all categorical variables against the target variable\n",
    "cat_cols = [x for x in df.select_dtypes(include=['object', 'category']).columns if x not in id_col if x not in target_col]\n",
    "for col in cat_cols:\n",
    "    sns.catplot(x=col, y=\"churn\", kind=\"bar\", data=df, ci=None)\n",
    "    plt.axhline(y=0.265, ls='--', c='red')\n",
    "    plt.text(0,0.27, \"Average churn rate in the population\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\" \\n\\n\\n\\n Churn rate by categories of '{}'\".format(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "\n",
    "We are interested in the categories of variables that exhibit a higher than average churn rate, i.e. the bars in the plots that cross the dotted line, which represents the average churn rate (i.e. ~27%)\n",
    "\n",
    "Observations:\n",
    "\n",
    "Customer segments that have a significantly higher churn rate based on the bar plots above-\n",
    "\n",
    "+ No partners (eg. single, separated, widow etc ...)\n",
    "+ No dependents (eg. either single, married with no kids...)\n",
    "+ Senior Citizens\n",
    "+ Multiple lines\n",
    "+ Fiber Optic\n",
    "+ No online security/online backup/device protection/techsupport\n",
    "+ With month-to-month subscription\n",
    "+ No paperless billing (likely electronic)\n",
    "+ Electronic payment\n",
    "\n",
    "The business or telecom SME can help validate initial findings, but the initial findings look intuitive. Customers who are likely to churn are those who are/have-- \n",
    "- young or retired singles\n",
    "- digitally savvy (electronic billing, payments etc.)\n",
    "- shorter contract periods (monthly subscription)\n",
    "- fewer features or add-ons on their products (no security, backup etc.)\n",
    "- experienced issues with internet service esp. Fiber Optic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - discrete and continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram plots for all discrete and continuous variables\n",
    "num_cols = [x for x in df.select_dtypes(include=['int64','float']).columns if x not in id_col if x not in target_col]\n",
    "for col in num_cols:\n",
    "    plt.title(\"\\n\\n\\n\\n Distribution plot for {}\".format(col))\n",
    "    plt.hist(df[col], bins = 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "\n",
    "+ Tenure is bimodal\n",
    "+ Monthly charges is skewed towards lower monthly charges\n",
    "+ Total charges has a long right tail and left skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for all discrete and continuous variables against the target variable\n",
    "num_cols = [x for x in df.select_dtypes(include=['int64','float']).columns if x not in id_col if x not in target_col]\n",
    "for col in num_cols:\n",
    "    sns.catplot(x=col, y=\"churn\", kind=\"box\", orient=\"h\", data=df)\n",
    "    plt.title(\"\\n\\n\\n\\n Box plot for '{}' by churn category (0/1)\".format(col))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "\n",
    "+ As expected lower tenure expected with higher churn rate\n",
    "+ Higher monthly charges associated with churn\n",
    "+ Interesting. Customers whose total charges are lesser tend to churn\n",
    "\n",
    "\n",
    "NOTE - You could further slice and dice the dataset and do a deep-dive in terms of EDA to extract additional insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "    \n",
    "Most machine learning algorithms expect non-null values in data, so it is important that input data for modelling doesn't have any missing values (i.e by applying an appropriate strategy)\n",
    "    \n",
    "The proportion of missing values for 'totalcharges' is insignificant (< 1%). We can choose to exclude these customers as it will not introduce any potential bias by doing so. Let's not remove any records, instead use simple imputation strategies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "\n",
    "The mean and median are quite far away from each other for totalcharges as we saw in the histogram. Since the distribution is left skewed, let's impute by median which is a better representation of central measure than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value imputation for totalcharges\n",
    "imr = Imputer(missing_values=np.NaN, strategy=\"median\")\n",
    "imr = imr.fit(df[['totalcharges']])\n",
    "df[\"totalcharges\"] = imr.transform(df[[\"totalcharges\"]]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check if missing values have been imputed\n",
    "df[\"totalcharges\"].isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection & treatment\n",
    "\n",
    "We will use tree-based GBMs (Gradient Boosting Machines) which are robust to outliers, instead of GLM (Generalized Linear Models), which are sensitive to extreme values in data. Although it depends on a lot of factors, without over-generalizing, GBMs are known to outperform GLMs. Hence we will not do any outlier treatment. Will revisit if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap outliers based on IQR\n",
    "# def winsorise(x):\n",
    "#     q1, q3 = np.percentile(x,(25,75))\n",
    "#     iqr = q3 - q1\n",
    "#     limit = q3 + 1.5*iqr\n",
    "#     return [min(i, limit) for i in x]\n",
    "\n",
    "# df_w = df.loc[:,['totalcharges','monthlycharges']].apply(winsorise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary \n",
    "Feature engineering is the process of deriving additional variables from the data that could be useful in predicting the likelihood of certain activity (eg. churn). Domain expertise and understanding of the business is vital to craft additional features. \n",
    "\n",
    "Note - We could employ automated feature engineering packages - featuretools but will avoid as the process is computationally expensive and also will have to regularize (drop redundant features) later if we have too many features that are hard to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a new feature to binarize Internet service\n",
    "def f(x):\n",
    "  if x['internetservice'] == 'No': return 1\n",
    "  else: return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the above function\n",
    "df['feat_internet_service'] = df.apply(f, axis=1)\n",
    "df['feat_internet_service'].value_counts()\n",
    "\n",
    "# Replace values of categorical variables to avoid confusion when converted to dummies\n",
    "df.replace(\"No internet service\", \"no_internet\", inplace=True)\n",
    "df.replace(\"No phone service\", \"no_phone\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bunch of simple features based on certain combinations i.e service and features. These may or may not be useful\n",
    "df['feat_gpd'] = df['gender'] + \"_Partner_\" + df['partner'] + \"_Dependents_\" + df['dependents'] # gender + partners + dependents\n",
    "df['feat_gs'] = df['gender'] + \"_Sr_Citizen_\" + df['seniorcitizen']  # gender + senior citizen\n",
    "df['feat_gscp'] = df['gender'] + \"_Sr_Citizen_\" + df['seniorcitizen'] + \"_Partner_\" + df['partner'] # gender + senior citizen +partner\n",
    "df['feat_sb'] = \"Security_\" + df['onlinesecurity'] + \"_Backup_\" + df['onlinebackup'] # Online security + Backup\n",
    "df['feat_sbd'] = \"Security_\" + df['onlinesecurity'] + \"_Backup_\" + df['onlinebackup'] + \"_DeviceProtect_\" + df['deviceprotection'] # Online security + Online backup + Device protection\n",
    "df['feat_sbdt'] = \"Security_\" + df['onlinesecurity'] + \"_Backup_\" + df['onlinebackup'] + \"_DeviceProtect_\" + df['deviceprotection'] + \"_TechSupport_\" + df['techsupport'] # Online security + Online backup + Device protection + Tech Support\n",
    "df['feat_stream'] =  \"StreamingTV_\" + df['streamingtv'] + \"_StreamingMovies_\" + df['streamingmovies'] # Streaming TV + Streaming Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for the charges variable\n",
    "df['feat_charges_ratio'] = df['monthlycharges']/df['totalcharges'] # Monthly charges as ratio of total charges\n",
    "df['feat_charges_delta'] = df['totalcharges'] - df['monthlycharges'] # Difference in total and monthly charges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding or dummification of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary \n",
    "\n",
    "Most Machine learning algorithms cannot work with categorical data so they need to be converted to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert all categorical variables into a list\n",
    "cat_cols = [x for x in df.select_dtypes(include=['object', 'category']).columns if x not in id_col if x not in target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify all categorical variables (create n-1 dummies for n levels of a categorical variable to avoid perfect collinearity)\n",
    "df_ohe = pd.get_dummies(data=df, columns=cat_cols, drop_first=True)\n",
    "# df_ohe_copy = df_ohe.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for target leakage and correlations (collinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between target variable - Churn and the features\n",
    "corr_matrix = pd.DataFrame(df_ohe[df_ohe.columns[1:]].corr()['churn'][:])\n",
    "corr_matrix.drop('churn',axis=0,inplace=True)\n",
    "corr_matrix.columns = ['Corr_coeff']\n",
    "corr_matrix = corr_matrix.iloc[corr_matrix['Corr_coeff'].abs().argsort()]\n",
    "corr_matrix.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "\n",
    "Models need to be simple. Having too many redundant features makes models suffer from overfitting (i.e. model will not be able to generalize well on unseen data) and make the model less interpretable. Also, any variable that is highly correlated to target variable should be dropped as this could be self-defining.\n",
    "\n",
    "Good read on data/target leakage - https://www.datarobot.com/wiki/target-leakage/\n",
    "\n",
    "Correlation coefficients suggest that the variable with the highest correlation is tenure (negative correlation) but is not strong so there is no evidence of any target leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find highly correlated features\n",
    "features = [x for x in df_ohe.columns if x not in id_col if x not in target_col]\n",
    "df_feat_sel = df_ohe[features]\n",
    "\n",
    "fs_coll = FeatureSelector(data=df_feat_sel, labels = features)\n",
    "fs_coll.identify_collinear(correlation_threshold = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for variables that are highly collinear (>0.9)\n",
    "fs_coll.plot_collinear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of collinear features that we will remove\n",
    "collinear_features = fs_coll.ops['collinear']\n",
    "\n",
    "# Dataframe of collinear features\n",
    "fs_coll.record_collinear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of redundant features to be dropped\n",
    "drop_feat = fs_coll.record_collinear.drop_feature.unique().tolist()\n",
    "drop_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary \n",
    "\n",
    "Looks like a majority of new features derived as part of feature engineering are useless!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant features from the dataframe\n",
    "df_ohe.drop(drop_feat,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "#### Remove predictors that have zero feature importance (i.e. no impact on the target variable - churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the appropriate parameters\n",
    "features = [x for x in df_ohe.columns if x not in id_col if x not in target_col]\n",
    "df_feat_sel = df_ohe[features]\n",
    "\n",
    "target_var = df_ohe['churn']\n",
    "fs_fi = FeatureSelector(data= df_feat_sel, labels = target_var)\n",
    "\n",
    "fs_fi.identify_zero_importance(task = 'classification', \n",
    "                            eval_metric = 'auc', \n",
    "                            n_iterations = 10, \n",
    "                             early_stopping = True)\n",
    "# list of zero importance features\n",
    "zero_importance_features = fs_fi.ops['zero_importance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature importances\n",
    "fs_fi.plot_feature_importances(threshold = 0.99, plot_n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "The feature derived feat_charges_ratio - Monthly charges as ratio of total charges turns out to be the second most important feature in predicting customer churn!\n",
    "\n",
    "There are 55 attributes that is able to explain maximum variation in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_importance_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove zero importance features from the dataframe\n",
    "# df_ohe.drop(zero_importance_features,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify features with low importance\n",
    "fs_fi.identify_low_importance(cumulative_importance = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "Let's not drop features with low importance for now although their contribution is marginal. \n",
    "\n",
    "You can come back to this step and check what happens if we remove these features and refit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check features with 0 variance\n",
    "fs_fi.identify_single_unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate plots to check consistency of features when data is split into train and holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for univariate plots\n",
    "modelling_df = df_ohe.iloc[:,1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for univariate plots\n",
    "data_train, data_holdout = train_test_split(modelling_df,test_size=0.2,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots for univariate analysis\n",
    "get_univariate_plots(data=data_train,target_col='churn',data_test = data_holdout, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "Comparison of feature trends in train and holdout\n",
    "\n",
    "Featexp calculates two metrics to display on these plots which help with gauging noisiness:\n",
    "\n",
    "Trend correlation(seen in test plot): If a feature doesn’t hold same trend w.r.t. target across train and evaluation sets, it can lead to overfitting. This happens because the model is learning something which is not applicable in test data. Trend correlation helps understand how similar train/test trends are and mean target values for bins in train & test are used to calculate it. \n",
    "\n",
    "Trend changes: Sudden and repeated changes in trend direction could imply noisiness. But, such trend change can also happen because that bin has a very different population in terms of other features and hence, its incidence rate can’t really be compared with other bins.\n",
    "\n",
    "SOURCE - https://www.kdnuggets.com/2018/11/secret-sauce-top-kaggle-competition.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats to check for correlation trends between train and test\n",
    "stats = get_trend_stats(data=data_train, target_col='churn', data_test=data_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain correlation trend stats\n",
    "stats_low_corr = stats.loc[stats['Trend_correlation'] < 0.8,:]\n",
    "stats_low_corr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These features will be removed prior to train and holdout split as there is no correlation or a perfect positive/negative correlation\n",
    "low_corr_trend_feat = stats_low_corr.Feature.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and holdout for model build and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframes for train and test split\n",
    "features = [x for x in df_ohe.columns if x not in id_col if x not in target_col if x not in low_corr_trend_feat]\n",
    "features_with_cust = [x for x in df_ohe.columns if x not in target_col if x not in low_corr_trend_feat]\n",
    "X = df_ohe[features]\n",
    "X_with_cust = df_ohe[features_with_cust]\n",
    "y = df_ohe[target_col]\n",
    "ID = df_ohe[id_col]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed data to csv\n",
    "# all_col = id_col + features + target_col\n",
    "# processed_df = df_ohe[all_col]\n",
    "# processed_df.to_csv('C:/Users/Lenovo/Desktop/processed_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and holdout\n",
    "\n",
    "X_train_cust,X_holdout_cust,y_train,y_holdout = train_test_split(X_with_cust,y,test_size=0.3)\n",
    "\n",
    "X_train = X_train_cust.drop(id_col,axis=1).reset_index(drop=True)\n",
    "X_holdout = X_holdout_cust.drop(id_col,axis=1).reset_index(drop=True)\n",
    "\n",
    "train_cust = X_train_cust[id_col].reset_index(drop=True)\n",
    "holdout_cust = X_holdout_cust[id_col].reset_index(drop=True)\n",
    "\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_holdout = y_holdout.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target distribution in train sample\")\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target distribution in holdout sample\")\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "The distribution of target variable in train and holdout samples look similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "\n",
    "We will use XGBoost for model build, without much hyper-parameter tuning. You can experiment with any binary classification algorithm you like - Logistic Regression, Support Vector Machines, Decision trees (CHAID, CART), Random Forest, GBM variants, Neural Networks, KNN etc.\n",
    "\n",
    "The model performance metric (eg. AUC-ROC, AUC-PR, log-loss, accuracy etc.) depends on the objective.\n",
    "\n",
    "We will consider AUC-ROC (ability of model to rank order well) as our primary model performance metric for this problem. We are interested in the model clearly separating the churners from no-churners or in other words exhibits good discriminatory power (rank-ordering ability). AUC-ROC is an appropriate metric for this purpose.\n",
    "\n",
    "Refer to this article for a list of model performance metrics - https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGboost with default parameters with 5 fold CV\n",
    "model = XGBClassifier()\n",
    "kfold = KFold(n_splits=5, random_state=3)\n",
    "cv_results = cross_val_score(model,X_train,y_train, cv=kfold,scoring='roc_auc')\n",
    "print (\"The mean AUC-ROC on 5 fold cross validation is {AUC}\".format(AUC = cv_results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "The mean AUC-ROC on 5 fold cross validation is 0.84. AUC-ROC ranges from 0.5 to 1. Higher the better. Models with AUC-ROC > 0.7 are considered to be fair. Based on the CV scores, model performance looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the XGBoostCalssifier on train data\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_holdout, y_holdout)], \n",
    "            early_stopping_rounds=100, eval_metric='auc', verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on holdout sample\n",
    "predictions = model.predict(X_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model on hold-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUC-ROC on hold-out sample\n",
    "predictions_probas = model.predict_proba(X_holdout)[:,1]\n",
    "print(\"AUC-ROC score on holdout sample is :\", round(roc_auc_score(y_holdout, predictions_probas),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predicted probabilities into a dataframe\n",
    "preds = pd.DataFrame(predictions_probas,columns=['pred_proba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach customer numbers to predicted probabilities \n",
    "holdout_pred_proba_cust = pd.concat([holdout_cust,preds],axis=1)\n",
    "holdout_pred_proba_cust.set_index('customerid', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "The model generalizes well on hold-out sample with AUC-ROC of 0.85 and is comparable to that of train data. No evidence of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC ROC\n",
    "fpr, tpr, thresh = roc_curve(y_holdout, predictions_probas, pos_label=1)\n",
    "\n",
    "random_probs = [0 for i in range(len(y_holdout))]\n",
    "p_fpr, p_tpr, _ = roc_curve(y_holdout, random_probs, pos_label=1)\n",
    "\n",
    "# plot roc curves\n",
    "plt.plot(fpr, tpr, linestyle='--',color='skyblue', label='XGBoost Classifier')\n",
    "plt.plot(p_fpr, p_tpr, linestyle='--', color='red', label='No model (Random)')\n",
    "# title\n",
    "plt.title('ROC curve')\n",
    "# x label\n",
    "plt.xlabel('False Positive Rate')\n",
    "# y label\n",
    "plt.ylabel('True Positive rate')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('ROC',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "    \n",
    "Good read on AUC-ROC and its interpretation - https://towardsdatascience.com/understanding-the-roc-and-auc-curves-a05b68550b69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy on holdout sample\n",
    "print(\"Accuracy score on holdout sample is\", round(accuracy_score(y_holdout, predictions),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, Recall and F1 score on holdout sample\n",
    "print(classification_report(y_holdout,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plot_importance(model, max_num_features=15, height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model for future use\n",
    "file_name = \"model_churn.pkl\"\n",
    "\n",
    "# save\n",
    "pickle.dump(model, open(file_name, \"wb\"))\n",
    "\n",
    "# load\n",
    "# model_loaded = pickle.load(open(file_name, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "    \n",
    "The analysis below will help identify the customers who the company will have to retain as they are likely to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decile tables, Lift and Gains charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for plotting\n",
    "\n",
    "def plot_pandas_style(styler):\n",
    "    from IPython.core.display import HTML\n",
    "    html = '\\n'.join([line.lstrip() for line in styler.render().split('\\n')])\n",
    "    return HTML(html)\n",
    "\n",
    "def highlight_max(s,color='red'):\n",
    "    '''\n",
    "    highlight the maximum in a Series red.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: {}'.format(color) if v else '' for v in is_max]\n",
    "\n",
    "def decile_labels(agg1,label,color='skyblue'):\n",
    "    agg_dummy = pd.DataFrame(OrderedDict((('TOTAL',0),('TARGET',0),('NONTARGET',0),('PCT_TAR',0),('CUM_TAR',0),('CUM_NONTAR',0),('DIST_TAR',0),('DIST_NONTAR',0),('SPREAD',0))),index=[0])\n",
    "    agg1 = agg1.append(agg_dummy).sort_index()\n",
    "    agg1.index.name = label\n",
    "    agg1 = agg1.style.apply(highlight_max, color = 'red', subset=['SPREAD'])\n",
    "    agg1.bar(subset=['TARGET'], color='{}'.format(color))\n",
    "    agg1.bar(subset=['TOTAL'], color='{}'.format(color))\n",
    "    agg1.bar(subset=['PCT_TAR'], color='{}'.format(color))\n",
    "    return(agg1)\n",
    "\n",
    "def deciling(data,decile_by,target,nontarget):\n",
    "    inputs = list(decile_by)\n",
    "    inputs.extend((target,nontarget))\n",
    "    decile = data[inputs]\n",
    "    grouped = decile.groupby(decile_by)\n",
    "    agg1 = pd.DataFrame({},index=[])\n",
    "    agg1['TOTAL'] = grouped.sum()[nontarget] + grouped.sum()[target]\n",
    "    agg1['TARGET'] = grouped.sum()[target]\n",
    "    agg1['NONTARGET'] = grouped.sum()[nontarget]\n",
    "    agg1['PCT_TAR'] = grouped.mean()[target]*100\n",
    "    agg1['CUM_TAR'] = grouped.sum()[target].cumsum()\n",
    "    agg1['CUM_NONTAR'] = grouped.sum()[nontarget].cumsum()\n",
    "    agg1['DIST_TAR'] = agg1['CUM_TAR']/agg1['TARGET'].sum()*100\n",
    "    agg1['DIST_NONTAR'] = agg1['CUM_NONTAR']/agg1['NONTARGET'].sum()*100\n",
    "    agg1['SPREAD'] = (agg1['DIST_TAR'] - agg1['DIST_NONTAR'])\n",
    "    agg1 = decile_labels(agg1,'DECILE',color='skyblue')\n",
    "    return(plot_pandas_style(agg1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data to calculate churn rate by decile\n",
    "scores_holdout = pd.DataFrame(model.predict_proba(X_holdout)[:,1], columns = ['SCORE'])\n",
    "scores_holdout['DECILE'] = pd.qcut(scores_holdout['SCORE'].rank(method = 'first'),10,labels=range(10,0,-1))\n",
    "scores_holdout['DECILE'] = scores_holdout['DECILE'].astype(int)\n",
    "scores_holdout['TARGET'] = y_holdout.values\n",
    "scores_holdout['NONTARGET'] = 1 - y_holdout.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print decile table for holdout sample\n",
    "print(\"Decile table for holdout sample\")\n",
    "deciling(scores_holdout,['DECILE'],'TARGET','NONTARGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "The table above is generated by rank-ordering customers by the predicted probabilities and splitting the sample into approx. 10 equal groups. Decision on how to implement the model is based on this table.\n",
    "\n",
    "TOTAL = Total customers in that decile\n",
    "TARGET = Churner, NON TARGET = Non-churner \n",
    "PCT_TAR = Proportion of churners in the decile\n",
    "CUM_TAR = Cumulative number of churners\n",
    "CUM_NON_TAR = Cumulative number of non-churners\n",
    "DIST_TAR = Distribution of churners (# Churners in the decile/Total # Churners)\n",
    "DIST_NONTAR = Distribution of non-churners (# Non-Churners in the decile/Total # Non-Churners)\n",
    "SPREAD \n",
    "\n",
    "Model is rank ordering very well on holdout sample without any discontinuity. ~70% of churners are captured within the first 3 deciles. A cutoff at either the 3rd -- ~70% churners or 4th decile -- ~80% churners will be optimum depending on marketing budget, contact strategy, retention unit capacity etc. Below the 4th decile, it is a point of diminishing return. This is because the average churn rate in the population is ~27%, however the churn rate for decile 5 is less than this ~24%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for plotting lift, PvO and gain charts\n",
    "def plots(agg1,target,type):\n",
    "\n",
    "    plt.figure(1,figsize=(20, 5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.plot(agg1['DECILE'],agg1['ACTUAL'],label='Actual')\n",
    "    plt.plot(agg1['DECILE'],agg1['PRED'],label='Pred')\n",
    "    plt.xticks(range(10,110,10))\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.grid(True)\n",
    "    plt.title('Actual vs Predicted', fontsize=20)\n",
    "    plt.xlabel(\"Population %\",fontsize=15)\n",
    "    plt.ylabel(str(target) + \" \" + str(type) + \" %\",fontsize=15)\n",
    "\n",
    "    plt.subplot(132)\n",
    "    X = agg1['DECILE'].tolist()\n",
    "    X.append(0)\n",
    "    Y = agg1['DIST_TAR'].tolist()\n",
    "    Y.append(0)\n",
    "    plt.plot(sorted(X),sorted(Y))\n",
    "    plt.plot([0, 100], [0, 100],'r--')\n",
    "    plt.xticks(range(0,110,10))\n",
    "    plt.yticks(range(0,110,10))\n",
    "    plt.grid(True)\n",
    "    plt.title('Gains Chart', fontsize=20)\n",
    "    plt.xlabel(\"Population %\",fontsize=15)\n",
    "    plt.ylabel(str(target) + str(\" DISTRIBUTION\") + \" %\",fontsize=15)\n",
    "    plt.annotate(round(agg1[agg1['DECILE'] == 30].DIST_TAR.item(),2),xy=[30,30], \n",
    "            xytext=(25, agg1[agg1['DECILE'] == 30].DIST_TAR.item() + 5),fontsize = 13)\n",
    "    plt.annotate(round(agg1[agg1['DECILE'] == 50].DIST_TAR.item(),2),xy=[50,50], \n",
    "            xytext=(45, agg1[agg1['DECILE'] == 50].DIST_TAR.item() + 5),fontsize = 13)\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.plot(agg1['DECILE'],agg1['LIFT'])\n",
    "    plt.xticks(range(10,110,10))\n",
    "    plt.grid(True)\n",
    "    plt.title('Lift Chart', fontsize=20)\n",
    "    plt.xlabel(\"Population %\",fontsize=15)\n",
    "    plt.ylabel(\"Lift\",fontsize=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def gains(data,decile_by,target,score):\n",
    "    inputs = list(decile_by)\n",
    "    inputs.extend((target,score))\n",
    "    decile = data[inputs]\n",
    "    grouped = decile.groupby(decile_by)\n",
    "    agg1 = pd.DataFrame({},index=[])\n",
    "    agg1['ACTUAL'] = grouped.mean()[target]*100\n",
    "    agg1['PRED'] = grouped.mean()[score]*100\n",
    "    agg1['DIST_TAR'] = grouped.sum()[target].cumsum()/grouped.sum()[target].sum()*100\n",
    "    agg1.index.name = 'DECILE'\n",
    "    agg1 = agg1.reset_index()\n",
    "    agg1['DECILE'] = agg1['DECILE']*10\n",
    "    agg1['LIFT'] = agg1['DIST_TAR']/agg1['DECILE']\n",
    "    plots(agg1,target,'Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot charts for holdout sample\n",
    "lift_holdout = pd.concat([X_holdout,scores_holdout],axis=1)\n",
    "print(\"Charts for holdout sample\")\n",
    "gains(lift_holdout,['DECILE'],'TARGET','SCORE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary \n",
    "\n",
    "Model seems to be under-predicting a bit in the initial deciles. This will likely improve with fine-tuning model, so we will proceed without spending too much on trying to improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target plots for top 5 features\n",
    "fea_imp = pd.DataFrame(list(model.get_booster().get_fscore().items()), columns = ['feature','importance']).sort_values('importance',ascending= False)\n",
    "top_5_feat = fea_imp.iloc[:5,0].tolist()\n",
    "top_5_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target plots for top 5 features\n",
    "for _ in top_5_feat:\n",
    "    fig, axes, summary_df = info_plots.target_plot(\n",
    "    df=data_train, feature=_, feature_name=_,\n",
    "    target=target_col, show_percentile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "Following are the high risk categories exhibiting likelihood to churn\n",
    "\n",
    "- monthlycharges -  monthly charges between 65 to 100 USD \n",
    "- feat_charges_ratio - high monthly to total charges ratio (> 0.07)\n",
    "- totalcharges - annual charges less than 325 $\n",
    "- contract_Two year - customers NOT on a 2 year contract\n",
    "- tenure - customer tenure less than a year (< 14 months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary \n",
    "\n",
    "There is a saying in the ML space. A model that is not interpretable is like a drug bottle without a label.\n",
    "\n",
    "Machine learning models are less interpretable (black-box). Model outputs are less useful if the end-user (call centre agent/representative) doesn't have context about a lead. For the agent to drive the conversation and retain a customer (s)he needs to understand why a customer is likely to churn. This is where a technique called SHAP is very useful to make the models explainable and the outputs consumable \n",
    "\n",
    "https://www.kaggle.com/diegovicente/using-shap-values-for-interpretability\n",
    "\n",
    "https://shap.readthedocs.io/en/latest/example_notebooks/general/Explainable%20AI%20with%20Shapley%20Values.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object that can calcuate SHAP values\n",
    "explainer = shap.TreeExplainer(model=model)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with SHAP values for all features in model\n",
    "shap_xgb_mdl = pd.DataFrame(shap_values,columns = X_holdout.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add customer number to the SHAP values dataframe\n",
    "shap_xgb_mdl['customerid'] = holdout_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert all features into a list\n",
    "vars = [x for x in shap_xgb_mdl.columns if x not in id_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create intermediate dataframe by transforming (melting) and assigning ranks\n",
    "intermed_1 = pd.melt(shap_xgb_mdl,id_vars = id_col, value_vars = vars, var_name ='Feature', value_name = 'SHAP_Feature')\n",
    "# intermed_1['RANK'] = intermed_1['SHAP_Feature'].abs().groupby(intermed_1['cust_no']).rank(method='first',ascending=False).astype(int)\n",
    "intermed_1['RANK'] = intermed_1['SHAP_Feature'].groupby(intermed_1['customerid']).rank(method='first',ascending=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another intermediate dataframe to get the feature values\n",
    "intermed_2 = pd.melt(X_holdout_cust, id_vars = id_col, value_vars = vars, var_name = 'Feature', value_name = 'Feature_Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the intermediate dataframes created above\n",
    "holdout_feat_shap_vals = intermed_1.merge(intermed_2,on=['customerid','Feature'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary \n",
    "\n",
    "The output of the previous step is long. This is transposed in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output dataframe with features to be passed as insights to retention unit (call centre) to make informed convsersations with customers\n",
    "output = holdout_feat_shap_vals.pivot(index='customerid',columns='RANK', values = ['Feature', 'Feature_Value', 'SHAP_Feature'])\n",
    "output.columns = output.columns.get_level_values(0)+'_'+['{:}'.format(x) for x in output.columns.get_level_values(1)]\n",
    "suffix = ('_1','_2','_3','_4','_5') #Depending on desirable number of features\n",
    "output_final = output.loc[:,output.columns.str.endswith(suffix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To the output above, merge the corresponding predicted churn probabilities\n",
    "output_final_with_pred_proba = pd.concat([output_final,holdout_pred_proba_cust],axis=1).sort_values(by=['pred_proba'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign bins (split into 10) based on predicted probabilities. Bin 1 - customers with highest probability to churn, 10 - least probability to churn\n",
    "output_final_with_pred_proba['percentile'] = pd.qcut(output_final_with_pred_proba['pred_proba'], 10, labels=False)\n",
    "output_final_with_pred_proba['bin'] = 10 - output_final_with_pred_proba['percentile']\n",
    "output_final_with_pred_proba.drop(['percentile'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export output to csv. Avoid exporting to xls as there is a risk of records getting truncated due to row/size constraints\n",
    "# output_final_with_pred_proba.to_csv('C:/Users/Lenovo/Desktop/output_raw.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring the output to a shape that can easily be consumed by agents\n",
    "column_titles = ['pred_proba','bin','Feature_1','Feature_Value_1','Feature_2','Feature_Value_2','Feature_3','Feature_Value_3']\n",
    "output_for_retention_unit = output_final_with_pred_proba.reindex(columns=column_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of 5 customers \n",
    "# Unless you set a seed the output will be different each time\n",
    "output_for_retention_unit.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary \n",
    "\n",
    "Let's take an example in the table above with a sample of customers to explain how this output is useful for an agent. Based on the decile analysis, we know that customers in bins 1 to 4 are in the high risk category. So, the telecom client will send only those customers who fall in bins 1 to 4 as leads to the retention unit.\n",
    "\n",
    "Customer ID 3247-ZVOUO falls in bin 1 with a predicted probability of ~0.75. Based on the SHAP values, this customer is very likely to churn because of the following reasons in that order -\n",
    "\n",
    "1. Proportion of monthly to total charges is high (~0.1) - Price\n",
    "2. Interent serviced through fiber optic (0=No,1=Yes) - Service\n",
    "3. Not on a 2 year contract (0=No,1=Yes) - Loyalty\n",
    "\n",
    "Based on these reasons, the agent can then have an informed conversation with customer and resolve any issues which can in turn help with retention (for eg. offering a discount, resolving technical issues immediately, offering a product/feature for free for a limited time period etc.).\n",
    "\n",
    "Similarly, the same can be extended to other leads too. Following this approach hekps with personalization based on customer profile, needs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export output to csv. Avoid exporting to xls as there is a risk of records getting truncated due to row/size constraints\n",
    "output_for_retention_unit.to_csv('C:/Users/Lenovo/Desktop/output_for_retention_unit.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directional impact of features on Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_viz(df_shap,df):\n",
    "\n",
    "    # Make a copy of the input data\n",
    "    shap_v = pd.DataFrame(df_shap)\n",
    "    feature_list = df.columns\n",
    "    shap_v.columns = feature_list\n",
    "    df_v = df.copy().reset_index().drop('index',axis=1)\n",
    "    \n",
    "    # Determine the correlation in order to plot with different colors\n",
    "    corr_list = list()\n",
    "    for i in feature_list:\n",
    "        b = np.corrcoef(shap_v[i],df_v[i])[1][0]\n",
    "        corr_list.append(b)\n",
    "    corr_df = pd.concat([pd.Series(feature_list),pd.Series(corr_list)],axis=1).fillna(0)\n",
    "    \n",
    "    # Make a data frame. Column 1 is the feature, and Column 2 is the correlation coefficient\n",
    "    corr_df.columns  = ['Variable','Corr']\n",
    "    corr_df['Sign'] = np.where(corr_df['Corr']>0,'skyblue','red')\n",
    "    \n",
    "    # Plot it\n",
    "    shap_abs = np.abs(shap_v)\n",
    "    k=pd.DataFrame(shap_abs.mean()).reset_index()\n",
    "    k.columns = ['Variable','SHAP_abs']\n",
    "    k2 = k.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')\n",
    "    k2 = k2.sort_values(by='SHAP_abs',ascending = False).head(10) # Change based on number of factors you are interested in\n",
    "    colorlist = k2['Sign']\n",
    "    ax = k2.plot.barh(x='Variable',y='SHAP_abs',color = colorlist, figsize=(5,6),legend=False)\n",
    "    ax.set_xlabel(\"SHAP Value (Skyblue = Positive Impact, Red = Negative Impact)\")\n",
    "    return k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP feature directional impact\n",
    "shap_xgb_mdl_wo_cust = shap_xgb_mdl.drop(['customerid'],axis=1)\n",
    "shap_viz(shap_xgb_mdl_wo_cust,X_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customers generally churn voluntarily due to the following reasons -\n",
    "1. Price (switch)\n",
    "2. Quality of service\n",
    "3. Usage\n",
    "4. Add-ons, features\n",
    "\n",
    "## Whom to target -\n",
    "1. Clearly customers in the top 3 deciles (depends on appetite, costs, resources, exclusions due to marketing consent etc.) identified above have a churn rate i.e. 1.5 to 3 times of the average churn rate. \n",
    "The ROI will be higher by targeting these customers.\n",
    "2. The customers who tend to churn-\n",
    "+ pay higher than average monthly charges, \n",
    "+ have low tenure,\n",
    "+ senior citizens,\n",
    "+ likely digitally savvy (evident from mode of payments and paperless billing)\n",
    "+ are on short-term contract periods\n",
    "+ have a fiber optic connection (hypothesis - issues with service)\n",
    "\n",
    "## Recommendations -\n",
    "\n",
    "1. To price sensitive customers - Offer discounts based on Customer LifeTime Value and Tenure. Also recognise loyalty.\n",
    "2. Improve stickiness of customers by signing them up for long term contracts and cross sell additional services by \n",
    "bundling products/features (eg. providing online security and backup)\n",
    "3. Understand what issues Fiber Optic customers are facing and resolve them on priority and follow up until issues are resolved.\n",
    "Tech support seems to be poor too, so that needs to be fixed.\n",
    "4. For younger, low tenure customers who generally have a tendency to switch frequently could think of offering high speed \n",
    "internet service during non-peak times (eg. gaming, binging on Netflix etc.) at no additional cost.\n",
    "\n",
    "We could think of overlaying the model outputs with segmentation analysis to get deeper insight on customer's behaviour.\n",
    "\n",
    "\n",
    "## Additional comments -\n",
    "+ A one size fits all approach will likely not be effective for retention unlike a personalised approach\n",
    "+ Recommend passing on insights to call centre centre team for retention to have personlized comms with customer based on \n",
    "breakdown of individual prediction for each customer based on SHAP. For eg. a customer who is price-sensitive might be offered a \n",
    "discount, while offering upgrades/additional services to a customer for whom the service and experience is important.\n",
    "+ Having temporal data over a period of time will help improve behavior of customer and likely improve performance of model\n",
    "by having aggregated features. This will also help with proactive retention when there are early signs of switching.\n",
    "+ Some of the additional data sources worth investigating.\n",
    "    - Demographics based on postal code of where customer lives (ABS)\n",
    "    - Complaints data (text mining)\n",
    "    - Competitor pricing\n",
    "    - Usage logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
